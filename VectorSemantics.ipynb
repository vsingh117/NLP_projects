{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.sparse\n",
    "import time\n",
    "import string\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE as tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data from files\n",
    "inp_eng = pd.read_csv('../data/CONcreTEXT_trial_EN.tsv',sep ='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TARGET POS  INDEX                                               TEXT  \\\n",
      "0   achievement   N      3  Bring up academic achievements , awards , and ...   \n",
      "1   achievement   N      9  Please list people you have helped , your pers...   \n",
      "2      activate   V      1     Add activated carbon straight to your vodka .    \n",
      "3      activate   V     15  Place sensors around your garden , and when a ...   \n",
      "4     adventure   N      9  Look for a partner that shares your level of a...   \n",
      "..          ...  ..    ...                                                ...   \n",
      "95        water   N      5  Rinse your face with warm water and pat it dry .    \n",
      "96          win   V      4  Staying mentally strong means winning half the...   \n",
      "97          win   V      7  The person who has the highest score wins the ...   \n",
      "98        woman   N      7  For the most part , men and women wear the sam...   \n",
      "99        woman   N      3  Look at the woman whom you are listening to fo...   \n",
      "\n",
      "    MEAN  \n",
      "0   3.06  \n",
      "1   3.03  \n",
      "2   3.83  \n",
      "3   5.51  \n",
      "4   2.03  \n",
      "..   ...  \n",
      "95  6.91  \n",
      "96  2.34  \n",
      "97  4.60  \n",
      "98  6.29  \n",
      "99  6.57  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(inp_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize words in a row\n",
    "punct = list(string.punctuation)\n",
    "\n",
    "def tokenize_speech(text):\n",
    "    return [word for word in word_tokenize(text) if not word in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_eng['Tokens'] = inp_eng.apply(lambda x: tokenize_speech(x['TEXT']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Bring, up, academic, achievements, awards, an...\n",
       "1     [Please, list, people, you, have, helped, your...\n",
       "2     [Add, activated, carbon, straight, to, your, v...\n",
       "3     [Place, sensors, around, your, garden, and, wh...\n",
       "4     [Look, for, a, partner, that, shares, your, le...\n",
       "                            ...                        \n",
       "95    [Rinse, your, face, with, warm, water, and, pa...\n",
       "96    [Staying, mentally, strong, means, winning, ha...\n",
       "97    [The, person, who, has, the, highest, score, w...\n",
       "98    [For, the, most, part, men, and, women, wear, ...\n",
       "99    [Look, at, the, woman, whom, you, are, listeni...\n",
       "Name: Tokens, Length: 100, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = inp_eng['Tokens']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utilities\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# For pretty-printing\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "UNK_TOKEN   = u\"<unk>\"\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "def pretty_print_matrix(M, rows=None, cols=None, dtype=float, float_fmt=\"{0:.04f}\"):\n",
    "    \"\"\"Pretty-print a matrix using Pandas.\n",
    "\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      float_fmt : format specifier for floats\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(M, index=rows, columns=cols, dtype=dtype)\n",
    "    old_fmt_fn = pd.get_option('float_format')\n",
    "    pd.set_option('float_format', lambda f: float_fmt.format(f))\n",
    "    display(df)\n",
    "    pd.set_option('float_format', old_fmt_fn)  # reset Pandas formatting\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "##\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "##\n",
    "# Data loading functions\n",
    "def get_corpus(name=\"brown\"):\n",
    "    import nltk\n",
    "    assert(nltk.download(name))\n",
    "    return nltk.corpus.__getattr__(name)\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    import vocabulary\n",
    "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
    "    \"\"\"Generate train/test split for unsupervised tasks.\n",
    "\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(list(corpus.sents()), dtype=object)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        rng.shuffle(sentences)  # in-place\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "def preprocess_sentences(sentences, vocab, use_eos=False, emit_ids=True):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "      use_eos: if true, will add </s> token to end of sentence.\n",
    "      emit_ids: if true, will emit as ids. Otherwise, will be preprocessed\n",
    "          tokens.\n",
    "\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    word_preproc = lambda w: canonicalize_word(w, wordset=vocab.word_to_id)\n",
    "    ret = []\n",
    "    for s in sentences:\n",
    "        canonical_words = vocab.pad_sentence(list(map(word_preproc, s)),\n",
    "                                             use_eos=use_eos)\n",
    "        ret.extend(vocab.words_to_ids(canonical_words) if emit_ids else\n",
    "                   canonical_words)\n",
    "    if not use_eos:  # add additional <s> to end if needed\n",
    "        ret.append(vocab.START_ID if emit_ids else vocab.START_TOKEN)\n",
    "    return np.array(ret, dtype=(np.int32 if emit_ids else object))\n",
    "\n",
    "\n",
    "def load_corpus(corpus, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\n",
    "\n",
    "    This is a convenience wrapper to chain together several functions from this\n",
    "    module, and produce a train/test split suitable for input to most models.\n",
    "\n",
    "    Sentences are preprocessed by canonicalization and converted to ids\n",
    "    according to the constructed vocabulary, and interspersed with <s> tokens\n",
    "    to denote sentence bounaries.\n",
    "\n",
    "    Args:\n",
    "        corpus: (string | corpus reader) If a string, will fetch the\n",
    "            NLTK corpus of that name.\n",
    "        split: (float \\in (0,1]) fraction of examples in train split\n",
    "        V: (int) vocabulary size (including special tokens)\n",
    "        shuffle: (int) if > 0, use as random seed to shuffle sentence prior to\n",
    "            split. Can change this to get different splits.\n",
    "\n",
    "    Returns:\n",
    "        (vocab, train_ids, test_ids)\n",
    "        vocab: vocabulary.Vocabulary object\n",
    "        train_ids: flat (1D) np.array(int) of ids\n",
    "        test_ids: flat (1D) np.array(int) of ids\n",
    "    \"\"\"\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = get_corpus(corpus)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids\n",
    "\n",
    "##\n",
    "# Window and batch functions\n",
    "def rnnlm_batch_generator(ids, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
    "    # Clip to multiple of max_time for convenience\n",
    "    clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "    input_w = ids[:clip_len]     # current word\n",
    "    target_y = ids[1:clip_len+1]  # next word\n",
    "    # Reshape so we can select columns\n",
    "    input_w = input_w.reshape([batch_size,-1])\n",
    "    target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "    # Yield batches\n",
    "    for i in xrange(0, input_w.shape[1], max_time):\n",
    "        yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "\n",
    "def build_windows(ids, N, shuffle=True):\n",
    "    \"\"\"Build window input to the window model.\n",
    "\n",
    "    Takes a sequence of ids, and returns a data matrix where each row\n",
    "    is a window and target for the window model. For N=3:\n",
    "        windows[i] = [w_3, w_2, w_1, w_0]\n",
    "\n",
    "    For language modeling, N is the context size and you can use y = windows[:,-1]\n",
    "    as the target words and x = windows[:,:-1] as the contexts.\n",
    "\n",
    "    For CBOW, N is the window size and you can use y = windows[:,N/2] as the target words\n",
    "    and x = np.hstack([windows[:,:N/2], windows[:,:N/2+1]]) as the contexts.\n",
    "\n",
    "    For skip-gram, you can use x = windows[:,N/2] as the input words and y = windows[:,i]\n",
    "    where i != N/2 as the target words.\n",
    "\n",
    "    Args:\n",
    "      ids: np.array(int32) of input ids\n",
    "      shuffle: if true, will randomly shuffle the rows\n",
    "\n",
    "    Returns:\n",
    "      windows: np.array(int32) of shape [len(ids)-N, N+1]\n",
    "        i.e. each row is a window, of length N+1\n",
    "    \"\"\"\n",
    "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
    "    for i in xrange(N+1):\n",
    "        # First column: first word, etc.\n",
    "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
    "    if shuffle:\n",
    "        # Shuffle rows\n",
    "        np.random.shuffle(windows)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\n",
    "\n",
    "    Args:\n",
    "      data: array-like, supporting slicing along first dimension\n",
    "      batch_size: int, batch size\n",
    "\n",
    "    Yields:\n",
    "      minibatches of maximum size batch_size\n",
    "    \"\"\"\n",
    "    for i in xrange(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check issue with following code\n",
    "#    print \"Loaded {:,} sentences ({:g} tokens)\".format(*fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Vocabulary helper functions\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  START_TOKEN = u\"<s>\"\n",
    "  END_TOKEN   = u\"</s>\"\n",
    "  UNK_TOKEN   = u\"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    \"\"\"Create a Vocabulary object.\n",
    "\n",
    "    Args:\n",
    "        tokens: iterator( string )\n",
    "        size: None for unlimited, or int > 0 for a fixed-size vocab.\n",
    "              Vocabulary size includes special tokens <s>, </s>, and <unk>\n",
    "    \"\"\"\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    self.bigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    word1 = None\n",
    "    for word in tokens:\n",
    "        if word1 is None:\n",
    "            pass\n",
    "        self.bigram_counts[word1][word] += 1\n",
    "        word1 = word\n",
    "    self.bigram_counts.default_factory = None  # make into a normal dict\n",
    "\n",
    "    # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.keys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def pad_sentence(self, words, use_eos=True):\n",
    "    ret = [self.START_TOKEN] + words\n",
    "    if use_eos:\n",
    "      ret.append(self.END_TOKEN)\n",
    "    return ret\n",
    "\n",
    "  def sentence_to_ids(self, words, use_eos=True):\n",
    "    return self.words_to_ids(self.pad_sentence(words, use_eos))\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title TSV Corpus Reader\n",
    "import sys, os\n",
    "\n",
    "class TSVCorpusReader(object):\n",
    "    \"\"\"Corpus reader for TSV files.\n",
    "\n",
    "    Input files are assumed to contain one sentence per line, with tokens\n",
    "    separated by tabs:\n",
    "\n",
    "    foo[tab]bar[tab]baz\n",
    "    span[tab]eggs\n",
    "\n",
    "    Would correspond to the two-sentence corpus:\n",
    "        [\"foo\", \"bar\", \"baz\"],\n",
    "        [\"spam\", \"eggs\"]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentence_file, preload=True, file_reader=open):\n",
    "        \"\"\"Construct a corpus reader for the given file.\n",
    "\n",
    "        Args:\n",
    "            sentence_file: (string) path to a TSV file with one sentence per\n",
    "                line.\n",
    "            preload: (bool) If true, will read entire corpus to memory on\n",
    "                construction. Otherwise, will load on-demand.\n",
    "            file_reader: (function string -> fd) optional replacement for\n",
    "                Python's built-in open(...) method, to be used for reading\n",
    "                from alternative file-like objects.\n",
    "        \"\"\"\n",
    "        self._open = file_reader\n",
    "        self._sentence_file = sentence_file\n",
    "        self._sentence_cache = []\n",
    "\n",
    "        if preload:\n",
    "            self._sentence_cache = list(self.sents())\n",
    "\n",
    "    def _line_iterator(self):\n",
    "        with self._open(self._sentence_file) as fd:\n",
    "            for line in fd:\n",
    "                yield line.strip()\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"Iterator over sentences in the corpus.\n",
    "\n",
    "        Yields:\n",
    "            list(string) of tokens\n",
    "        \"\"\"\n",
    "        if self._sentence_cache:\n",
    "            for sentence in self._sentence_cache:\n",
    "                yield sentence\n",
    "        else:\n",
    "            # If no cache, actually read the file.\n",
    "            for line in self._line_iterator():\n",
    "                yield line.split(\"\\t\")\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"Iterator over words in the corpus.\n",
    "\n",
    "        Yields:\n",
    "            (string) tokens\n",
    "        \"\"\"\n",
    "        for sentence in self.sents():\n",
    "            for word in sentence:\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocabulary\n",
    "\n",
    "Let's now get started with creating the vocabulary. We'll use some of the functions defined in the utility classes we just loaded above.\n",
    "\n",
    "(Note: the following code cell may take 20-30 seconds to complete running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 648 words\n",
      "Corpus: 1,416 tokens (counting <s>)\n",
      "Sample words: ['<s>' 'bring' 'up' 'academic' 'achievements' 'awards' 'and' 'other'\n",
      " 'milestones' 'in']\n",
      "Sample ids: [0, 79, 80, 171, 81]\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary by first canonicalizing all the words -- lowercasing\n",
    "# and converting all digits to a single string. The vocabulary maintains a\n",
    "# mapping between words and integer ids.\n",
    "vocab = Vocabulary(canonicalize_word(w)\n",
    "                   for w in flatten(corpus))\n",
    "print(\"Vocabulary: {:,} words\".format(vocab.size))\n",
    "\n",
    "# Turn the corpus into a single flattened list of tokens, where each sentence\n",
    "# begins with a special marker <s>.\n",
    "tokens = preprocess_sentences(corpus, vocab, use_eos=False, emit_ids=False)\n",
    "print(\"Corpus: {:,} tokens (counting <s>)\".format(len(tokens)))\n",
    "\n",
    "# Retrieve the ids corresponding to the tokens (above). This is the data\n",
    "# we'll actually use.\n",
    "token_ids = vocab.words_to_ids(tokens)\n",
    "print('Sample words:', tokens[:10])\n",
    "print('Sample ids:', token_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that produces a sparse co-occurrence matrix given a corpus,\n",
    "# a vocabulary size V, and K (the context window is +-K).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that produces a sparse co-occurrence matrix given a corpus,\n",
    "# a vocabulary size V, and K (the context window is +-K).\n",
    "def co_occurrence_matrix(token_ids, V, K=2):\n",
    "    # We'll use this as an \"accumulator\" matrix.\n",
    "    C = scipy.sparse.csc_matrix((V,V), dtype=np.float32)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        print(u'Counting pairs (i, i \\u00B1 %d) ...' %k)\n",
    "        i = token_ids[:-k]  # current word\n",
    "        j = token_ids[k:]   # k words ahead\n",
    "        data = (np.ones_like(i), (i,j))  # values, indices\n",
    "        Ck_plus = scipy.sparse.coo_matrix(data, shape=C.shape, dtype=np.float32)\n",
    "        Ck_plus = scipy.sparse.csc_matrix(Ck_plus)\n",
    "        Ck_minus = Ck_plus.T  # consider k words behind\n",
    "        C += Ck_plus + Ck_minus\n",
    "\n",
    "    print(\"Co-occurrence matrix: %d words x %d words\" %C.shape)\n",
    "    print(\"%.02g nonzero elements\" %C.nnz)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting pairs (i, i ± 1) ...\n",
      "Counting pairs (i, i ± 2) ...\n",
      "Counting pairs (i, i ± 3) ...\n",
      "Co-occurrence matrix: 648 words x 648 words\n",
      "7.1e+03 nonzero elements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>you</th>\n",
       "      <th>a</th>\n",
       "      <th>to</th>\n",
       "      <th>your</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>...</th>\n",
       "      <th>wins</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>wear</th>\n",
       "      <th>same</th>\n",
       "      <th>shoes</th>\n",
       "      <th>woman</th>\n",
       "      <th>whom</th>\n",
       "      <th>she</th>\n",
       "      <th>speaking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoes</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaking</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          <s>  </s>  <unk>  the  you   a  to  your  and  of  ...  wins  men  \\\n",
       "<s>         0     0      0   31   22  20   9    19    9  17  ...     1    0   \n",
       "</s>        0     0      0    0    0   0   0     0    0   0  ...     0    0   \n",
       "<unk>       0     0      0    0    0   0   0     0    0   0  ...     0    0   \n",
       "the        31     0      0    8   11   4  13    10    4  14  ...     2    1   \n",
       "you        22     0      0   11    2   5   6     2    4   2  ...     0    0   \n",
       "...       ...   ...    ...  ...  ...  ..  ..   ...  ...  ..  ...   ...  ...   \n",
       "shoes       1     0      0    0    0   0   0     0    0   1  ...     0    0   \n",
       "woman       0     0      0    1    1   0   0     0    0   0  ...     0    0   \n",
       "whom        0     0      0    1    1   0   0     0    0   0  ...     0    0   \n",
       "she         1     0      0    0    1   0   0     0    0   0  ...     0    0   \n",
       "speaking    1     0      0    0    0   0   0     0    0   0  ...     0    0   \n",
       "\n",
       "          women  wear  same  shoes  woman  whom  she  speaking  \n",
       "<s>           0     0     0      1      0     0    1         1  \n",
       "</s>          0     0     0      0      0     0    0         0  \n",
       "<unk>         0     0     0      0      0     0    0         0  \n",
       "the           1     1     1      0      1     1    0         0  \n",
       "you           0     0     0      0      1     1    1         0  \n",
       "...         ...   ...   ...    ...    ...   ...  ...       ...  \n",
       "shoes         0     0     1      0      0     0    0         0  \n",
       "woman         0     0     0      0      0     1    0         0  \n",
       "whom          0     0     0      0      1     0    0         0  \n",
       "she           0     0     0      0      0     0    0         1  \n",
       "speaking      0     0     0      0      0     0    1         0  \n",
       "\n",
       "[648 rows x 648 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the co-occurrence matrix.\n",
    "vec_C = co_occurrence_matrix(token_ids, vocab.size, K=3)\n",
    "\n",
    "# Display a table with the counts. The .toarray() function converts the\n",
    "# sparse matrix into a dense one.\n",
    "vec_labels = vocab.ordered_words()\n",
    "pretty_print_matrix(vec_C.toarray(), rows=vec_labels,\n",
    "                    cols=vec_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-1 Write a function to compute the PPMI matrix, which is a n-by-n matrix where each element is the PPMI value between two distinct words. \n",
    "#### Test your function using the first 100 sentences of the English language data from our class data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>you</th>\n",
       "      <th>a</th>\n",
       "      <th>to</th>\n",
       "      <th>your</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>...</th>\n",
       "      <th>wins</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>wear</th>\n",
       "      <th>same</th>\n",
       "      <th>shoes</th>\n",
       "      <th>woman</th>\n",
       "      <th>whom</th>\n",
       "      <th>she</th>\n",
       "      <th>speaking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.1894</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2463</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>1.2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3007</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0124</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoes</th>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whom</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3193</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6558</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.8679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaking</th>\n",
       "      <td>1.2627</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.8679</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            <s>   </s>  <unk>    the    you      a     to   your    and  \\\n",
       "<s>      0.0000 0.0000 0.0000 0.1481 0.1416 0.1894 0.0000 0.2463 0.0000   \n",
       "</s>     0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "<unk>    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "the      0.1481 0.0000 0.0000 0.0000 0.0000 0.0000 0.3007 0.0665 0.0000   \n",
       "you      0.1416 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "...         ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "shoes    0.8572 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "woman    0.0000 0.0000 0.0000 1.3193 1.6558 0.0000 0.0000 0.0000 0.0000   \n",
       "whom     0.0000 0.0000 0.0000 1.3193 1.6558 0.0000 0.0000 0.0000 0.0000   \n",
       "she      0.8572 0.0000 0.0000 0.0000 1.6558 0.0000 0.0000 0.0000 0.0000   \n",
       "speaking 1.2627 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "\n",
       "             of  ...   wins    men  women   wear   same  shoes  woman   whom  \\\n",
       "<s>      0.3232  ... 0.8572 0.0000 0.0000 0.0000 0.0000 0.8572 0.0000 0.0000   \n",
       "</s>     0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "<unk>    0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "the      0.5910  ... 2.0124 1.3193 1.3193 1.3193 1.3193 0.0000 1.3193 1.3193   \n",
       "you      0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 1.6558 1.6558   \n",
       "...         ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "shoes    2.0951  ... 0.0000 0.0000 0.0000 0.0000 5.4624 0.0000 0.0000 0.0000   \n",
       "woman    0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 5.4624   \n",
       "whom     0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 5.4624 0.0000   \n",
       "she      0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "speaking 0.0000  ... 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "\n",
       "            she  speaking  \n",
       "<s>      0.8572    1.2627  \n",
       "</s>     0.0000    0.0000  \n",
       "<unk>    0.0000    0.0000  \n",
       "the      0.0000    0.0000  \n",
       "you      1.6558    0.0000  \n",
       "...         ...       ...  \n",
       "shoes    0.0000    0.0000  \n",
       "woman    0.0000    0.0000  \n",
       "whom     0.0000    0.0000  \n",
       "she      0.0000    5.8679  \n",
       "speaking 5.8679    0.0000  \n",
       "\n",
       "[648 rows x 648 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PPMI(C):\n",
    "    \"\"\"Tranform a counts matrix to PPMI.\n",
    "    \n",
    "    Args:\n",
    "      C: scipy.sparse.csc_matrix of counts C_ij\n",
    "    \n",
    "    Returns:\n",
    "      (scipy.sparse.csc_matrix) PPMI(C) as defined above\n",
    "    \"\"\"\n",
    "    # Total count.\n",
    "    Z = float(C.sum())\n",
    "\n",
    "    # Sum each row (along columns).\n",
    "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten()\n",
    "    \n",
    "    # Get indices of relevant elements.\n",
    "    ii, jj = C.nonzero()  # row, column indices\n",
    "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
    "    \n",
    "    # PMI equation.\n",
    "    pmi = np.log(Cij * Z / (Zr[ii] * Zr[jj]))\n",
    "\n",
    "    # Truncate to positive only.\n",
    "    ppmi = np.maximum(0, pmi)  # take positive only\n",
    "    \n",
    "    # Re-format as sparse matrix.\n",
    "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
    "                                  dtype=np.float64)\n",
    "    ret.eliminate_zeros()  # remove zeros\n",
    "    return ret\n",
    "\n",
    "# Display the PPMI'd version of the co-occurrence matrix.\n",
    "pretty_print_matrix(PPMI(vec_C).toarray(), rows=vec_labels, \n",
    "                    cols=vec_labels, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-2) Briefly describe the algorithm for forming the PPMI matrix. What is the time complexity of your algorithm? Write at least 50 words.\n",
    "\n",
    "Ans :- Firstly I have created a matrix of word with co-occurences with a distance of K. In the above solution, I have used K=3. \n",
    "       1) Calculate total number of tokens in the corpus(Z)<br>\n",
    "       2) Calculate sum of rows accross all columns in the matrix. <br>\n",
    "       3) Get the indices of relevant non zero elements. <br>\n",
    "       4) Compute PMI for all elements in the matrix using following formula :- <br>\n",
    "          PMI(𝑖,𝑗)=log𝑃(𝑖,𝑗)𝑃(𝑖)𝑃(𝑗)=log𝐶𝑖𝑗⋅𝑍𝑍𝑖⋅𝑍𝑗 <br>\n",
    "       5) We consider all non zero values , this we do using a max function i.e. if PMI is greater than 0, we will consider teh PMI value, whereas if \n",
    "          calculated PMI is less than 0 then we consider 0 for such values. <br>\n",
    "          PPMI(𝑖,𝑗)=max(0,PMI(𝑖,𝑗))  --- Logic for ignoring negatively-correlated pairs <br>\n",
    "          \n",
    "Time complexity is the computational complexity that describes the amount of iterations it takes to run PPMI algorithm.\n",
    "In above solution vec_C is 2-D matrix of 648 rows × 648 columns hence, the function will be looped 648*648 times.\n",
    "\n",
    "Hence, the time complexity can be calculated as O(648 * 648).       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-3) How would you test if the representation of the words in your PPMI matrix reflects some fact about the relationship between words in the real world? \n",
    "###      For example, if two words are expected to co-occur together a lot, the PPMI value should be high (and vice versa). \n",
    "###      Write at least 50 words in your answer and give at least 2 pairs of examples from your PPMI matrix.\n",
    "\n",
    "#### Ans:- In english language we know that usually Subject and verbs are plaecd closed to each other. <br>\n",
    "#### In english generally, sentence starts with Noun/Pronoun or prepositions like \"The\". <br>\n",
    "\n",
    "#### Now I will compare the PPMI values for various pairs of words. <br>\n",
    "#### Firstly, PMI value for pair of Noun and other part of speech and I observe that the PPMI value is high:- <br>\n",
    "PMMI value for (Shoes,of) is ::  2.0951 <br>\n",
    "PMMI value for (Shoes,same) is ::  5.4624 <br>\n",
    "PMMI value for (she,speaking) is ::  5.8679 <br>\n",
    "\n",
    "#### PMMI value for pair of Subjects or subject,object and I observe that the PPMI value is low :- <br>\n",
    "PMMI value for (Shoes, men) is ::  0 <br>\n",
    "PMMI value for (shoes,she) is ::  0 <br>\n",
    "\n",
    "#### Now I will compare PPMI value for pair of \"start of line\" and Noun/Pronoun and I observe that the PPMI value is high <br>\n",
    "PMMI value for (\"start of line\",she) is :: 0.8572 <br>\n",
    "PMMI value for (\"start of line\",shoes) is :: 0.8572 <br>\n",
    "\n",
    "#### Now I will compare PPMI value for pair of \"start of line\" and conjunction/prepositions and I observe that the PPMI value is low <br> \n",
    "PMMI value for (\"start of line\",and) is :: 0 <br>\n",
    "PMMI value for (\"start of line\",to) is :: 0 <br>\n",
    "\n",
    "#### From above examples I observed that PPMI value is high between words which tend to be closer to eachother and PPMI is low between wordswhich tend to be far away from each other as per the generic english rules. <br>\n",
    "\n",
    "#### With this I can conclude that PPMI matrix generated reflects the relationship between words in english language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-4) Repeat Question 1 for the first 100 sentences in the Italian language data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_itl = pd.read_csv('../data/CONcreTEXT_trial_IT.tsv',sep='\\t')\n",
    "inp_itl['Tokens'] = inp_itl.apply(lambda x: tokenize_speech(x['TEXT']),axis=1)\n",
    "corpus_itl = inp_itl['Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 714 words\n",
      "Corpus: 1,411 tokens (counting <s>)\n",
      "Sample words: ['<s>' 'guardati' 'i' 'piedi' 'o' 'fai' 'finta' 'di' 'essere'\n",
      " 'affascinata']\n",
      "Sample ids: [0, 151, 11, 152, 20]\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary by first canonicalizing all the words -- lowercasing\n",
    "# and converting all digits to a single string. The vocabulary maintains a\n",
    "# mapping between words and integer ids.\n",
    "vocab_itl = Vocabulary(canonicalize_word(w)\n",
    "                   for w in flatten(corpus_itl))\n",
    "print(\"Vocabulary: {:,} words\".format(vocab_itl.size))\n",
    "\n",
    "# Turn the corpus into a single flattened list of tokens, where each sentence\n",
    "# begins with a special marker <s>.\n",
    "tokens = preprocess_sentences(corpus_itl, vocab_itl, use_eos=False, emit_ids=False)\n",
    "print(\"Corpus: {:,} tokens (counting <s>)\".format(len(tokens)))\n",
    "\n",
    "# Retrieve the ids corresponding to the tokens (above). This is the data\n",
    "# we'll actually use.\n",
    "token_ids = vocab_itl.words_to_ids(tokens)\n",
    "print('Sample words:', tokens[:10])\n",
    "print('Sample ids:', token_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting pairs (i, i ± 1) ...\n",
      "Counting pairs (i, i ± 2) ...\n",
      "Counting pairs (i, i ± 3) ...\n",
      "Co-occurrence matrix: 714 words x 714 words\n",
      "7.4e+03 nonzero elements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>di</th>\n",
       "      <th>e</th>\n",
       "      <th>la</th>\n",
       "      <th>un</th>\n",
       "      <th>in</th>\n",
       "      <th>il</th>\n",
       "      <th>per</th>\n",
       "      <th>...</th>\n",
       "      <th>propria</th>\n",
       "      <th>felicità</th>\n",
       "      <th>conigli</th>\n",
       "      <th>hanno</th>\n",
       "      <th>ottimo</th>\n",
       "      <th>udito</th>\n",
       "      <th>ottima</th>\n",
       "      <th>individuare</th>\n",
       "      <th>predatori</th>\n",
       "      <th>facilmente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>di</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>udito</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ottima</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individuare</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predatori</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facilmente</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             <s>  </s>  <unk>  di   e  la  un  in  il  per  ...  propria  \\\n",
       "<s>            0     0      0  12  11  20  13  10  11    7  ...        1   \n",
       "</s>           0     0      0   0   0   0   0   0   0    0  ...        0   \n",
       "<unk>          0     0      0   0   0   0   0   0   0    0  ...        0   \n",
       "di            12     0      0   8   9   8   8   6   4    4  ...        1   \n",
       "e             11     0      0   9   0   5   4   1   0    0  ...        0   \n",
       "...          ...   ...    ...  ..  ..  ..  ..  ..  ..  ...  ...      ...   \n",
       "udito          0     0      0   0   1   0   2   0   0    0  ...        0   \n",
       "ottima         0     0      0   0   2   0   1   0   0    0  ...        0   \n",
       "individuare    0     0      0   0   1   0   0   0   0    0  ...        0   \n",
       "predatori      1     0      0   0   0   0   0   0   0    0  ...        0   \n",
       "facilmente     1     0      0   0   0   0   0   0   0    0  ...        0   \n",
       "\n",
       "             felicità  conigli  hanno  ottimo  udito  ottima  individuare  \\\n",
       "<s>                 1        1      1       0      0       0            0   \n",
       "</s>                0        0      0       0      0       0            0   \n",
       "<unk>               0        0      0       0      0       0            0   \n",
       "di                  0        0      0       0      0       0            0   \n",
       "e                   0        0      0       1      1       2            1   \n",
       "...               ...      ...    ...     ...    ...     ...          ...   \n",
       "udito               0        0      1       1      0       1            0   \n",
       "ottima              0        0      0       0      1       0            0   \n",
       "individuare         0        0      0       0      0       0            0   \n",
       "predatori           0        0      0       0      0       0            1   \n",
       "facilmente          0        0      0       0      0       0            1   \n",
       "\n",
       "             predatori  facilmente  \n",
       "<s>                  1           1  \n",
       "</s>                 0           0  \n",
       "<unk>                0           0  \n",
       "di                   0           0  \n",
       "e                    0           0  \n",
       "...                ...         ...  \n",
       "udito                0           0  \n",
       "ottima               0           0  \n",
       "individuare          1           1  \n",
       "predatori            0           1  \n",
       "facilmente           1           0  \n",
       "\n",
       "[714 rows x 714 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the co-occurrence matrix.\n",
    "vec_itl = co_occurrence_matrix(token_ids, vocab_itl.size, K=3)\n",
    "\n",
    "# Display a table with the counts. The .toarray() function converts the\n",
    "# sparse matrix into a dense one.\n",
    "vec_labels = vocab_itl.ordered_words()\n",
    "pretty_print_matrix(vec_itl.toarray(), rows=vec_labels,\n",
    "                    cols=vec_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <th>di</th>\n",
       "      <th>e</th>\n",
       "      <th>la</th>\n",
       "      <th>un</th>\n",
       "      <th>in</th>\n",
       "      <th>il</th>\n",
       "      <th>per</th>\n",
       "      <th>...</th>\n",
       "      <th>propria</th>\n",
       "      <th>felicità</th>\n",
       "      <th>conigli</th>\n",
       "      <th>hanno</th>\n",
       "      <th>ottimo</th>\n",
       "      <th>udito</th>\n",
       "      <th>ottima</th>\n",
       "      <th>individuare</th>\n",
       "      <th>predatori</th>\n",
       "      <th>facilmente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4155</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>1.2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>di</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5271</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.9624</td>\n",
       "      <td>1.9624</td>\n",
       "      <td>2.6555</td>\n",
       "      <td>1.9624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>udito</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.9624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.8198</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4589</td>\n",
       "      <td>5.4589</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4589</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ottima</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.6555</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.1267</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4589</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individuare</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.9624</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.6412</td>\n",
       "      <td>5.8643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predatori</th>\n",
       "      <td>1.0360</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.6412</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facilmente</th>\n",
       "      <td>1.2592</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.8643</td>\n",
       "      <td>6.0467</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               <s>   </s>  <unk>     di      e     la     un     in     il  \\\n",
       "<s>         0.0000 0.0000 0.0000 0.0000 0.0000 0.4155 0.0865 0.1118 0.2071   \n",
       "</s>        0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "<unk>       0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "di          0.0000 0.0000 0.0000 0.0000 0.2278 0.1725 0.2743 0.2743 0.0000   \n",
       "e           0.0000 0.0000 0.0000 0.2278 0.0000 0.1378 0.0165 0.0000 0.0000   \n",
       "...            ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "udito       0.0000 0.0000 0.0000 0.0000 1.9624 0.0000 2.8198 0.0000 0.0000   \n",
       "ottima      0.0000 0.0000 0.0000 0.0000 2.6555 0.0000 2.1267 0.0000 0.0000   \n",
       "individuare 0.0000 0.0000 0.0000 0.0000 1.9624 0.0000 0.0000 0.0000 0.0000   \n",
       "predatori   1.0360 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "facilmente  1.2592 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "\n",
       "               per  ...  propria  felicità  conigli  hanno  ottimo  udito  \\\n",
       "<s>         0.0000  ...   0.8537    0.8537   0.8537 0.8537  0.0000 0.0000   \n",
       "</s>        0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "<unk>       0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "di          0.0000  ...   1.5271    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "e           0.0000  ...   0.0000    0.0000   0.0000 0.0000  1.9624 1.9624   \n",
       "...            ...  ...      ...       ...      ...    ...     ...    ...   \n",
       "udito       0.0000  ...   0.0000    0.0000   0.0000 5.4589  5.4589 0.0000   \n",
       "ottima      0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 5.4589   \n",
       "individuare 0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "predatori   0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "facilmente  0.0000  ...   0.0000    0.0000   0.0000 0.0000  0.0000 0.0000   \n",
       "\n",
       "             ottima  individuare  predatori  facilmente  \n",
       "<s>          0.0000       0.0000     1.0360      1.2592  \n",
       "</s>         0.0000       0.0000     0.0000      0.0000  \n",
       "<unk>        0.0000       0.0000     0.0000      0.0000  \n",
       "di           0.0000       0.0000     0.0000      0.0000  \n",
       "e            2.6555       1.9624     0.0000      0.0000  \n",
       "...             ...          ...        ...         ...  \n",
       "udito        5.4589       0.0000     0.0000      0.0000  \n",
       "ottima       0.0000       0.0000     0.0000      0.0000  \n",
       "individuare  0.0000       0.0000     5.6412      5.8643  \n",
       "predatori    0.0000       5.6412     0.0000      6.0467  \n",
       "facilmente   0.0000       5.8643     6.0467      0.0000  \n",
       "\n",
       "[714 rows x 714 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the PPMI'd version of the co-occurrence matrix.\n",
    "pretty_print_matrix(PPMI(vec_itl).toarray(), rows=vec_labels, \n",
    "                    cols=vec_labels, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
