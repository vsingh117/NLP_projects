{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data from files\n",
    "en_text = pd.read_csv('../data/CONcreTEXT_trial_EN.tsv',sep ='\\t')['TEXT']\n",
    "it_text = pd.read_csv('../data/CONcreTEXT_trial_IT.tsv',sep='\\t')['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "80\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Eng_train, Eng_test = train_test_split(en_text, test_size=0.20,\n",
    "                                       random_state=101)\n",
    "\n",
    "Itl_train, Itl_test = train_test_split(it_text, test_size=0.20,\n",
    "                                       random_state=101)\n",
    "print(len(en_text))\n",
    "print(len(Eng_train))\n",
    "print(len(Eng_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Etext_train = \"\"\n",
    "Etext_test = \"\"\n",
    "Itext_train = \"\"\n",
    "Itext_test = \"\"\n",
    "Etext = \"\"\n",
    "Itext = \"\"\n",
    "\n",
    "for i in Eng_train:\n",
    "    Etext_train+=i.lower()\n",
    "\n",
    "for i in Itl_train:\n",
    "    Itext_train+=i.lower()\n",
    "\n",
    "for i in en_text:\n",
    "    Etext+=i.lower()    \n",
    "\n",
    "for i in it_text:\n",
    "    Itext+=i.lower()\n",
    "#print(Etext_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data and vocab for bigram model\n",
    "train_bge, vocab_bge = padded_everygram_pipeline(2, Etext_train)\n",
    "train_bgi, vocab_bgi = padded_everygram_pipeline(2, Itext_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# English words: 336 \t # Italian Words: 288\n"
     ]
    }
   ],
   "source": [
    "# In my solution I converting all the words into lower case to maintain consistency\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "en_train = []\n",
    "en_words = []\n",
    "it_train = []\n",
    "it_words = []\n",
    "Efull = []\n",
    "Ifull = []\n",
    "\n",
    "for sent in Eng_train:\n",
    "    for w in word_tokenize(sent):\n",
    "        en_train.append(w)\n",
    "\n",
    "for sent in Eng_test:\n",
    "    for w in word_tokenize(sent):\n",
    "        en_words.append(w)\n",
    "        \n",
    "for sent in Itl_train:\n",
    "    for w in word_tokenize(sent):\n",
    "        it_train.append(w)\n",
    "                \n",
    "for sent in Itl_test:\n",
    "    for w in word_tokenize(sent):\n",
    "        it_words.append(w)\n",
    "        \n",
    "for sent in en_text:\n",
    "    for w in word_tokenize(sent):\n",
    "        Efull.append(w)\n",
    "        \n",
    "for sent in it_text:\n",
    "    for w in word_tokenize(sent):\n",
    "        Ifull.append(w)\n",
    "        \n",
    "print(u'# English words: {} \\t # Italian Words: {}'.format(len(en_words),len(it_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data and vocab for bigram model\n",
    "train_toke, vocab_toke = padded_everygram_pipeline(2, en_train)\n",
    "train_toki, vocab_toki = padded_everygram_pipeline(2, it_train)\n",
    "\n",
    "Etrain_inter, Evocab_inter =  padded_everygram_pipeline(2, en_train)\n",
    "Itrain_inter, Ivocab_inter =  padded_everygram_pipeline(2, it_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Etext_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building laplace model for bigram\n",
    "n = 2\n",
    "lpc_eng = Laplace(n)\n",
    "lpc_itl = Laplace(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text model fit\n",
    "lpc_eng.fit(train_bge, vocab_bge)\n",
    "lpc_itl.fit(train_bgi, vocab_bgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize model fit\n",
    "lpc_eng.fit(train_toke, vocab_toke)\n",
    "lpc_itl.fit(train_toki, vocab_toki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Laplace model for identifying English language in model is ::  0.6130952380952381\n"
     ]
    }
   ],
   "source": [
    "# Testing model for English language \n",
    "eng_cor = 0\n",
    "itl_cor = 0\n",
    "for words in  en_words:\n",
    "    pbl_eng = 1\n",
    "    pbl_itl = 1\n",
    "    for i in range(len(words)):\n",
    "        if(i< len(words)-1):\n",
    "            pbl_eng = pbl_eng * lpc_eng.score(words[i+1],[words[i]])\n",
    "            \n",
    "            pbl_itl = pbl_itl * lpc_itl.score(words[i+1],[words[i]])\n",
    "    \n",
    "    if(pbl_eng > pbl_itl):\n",
    "        eng_cor = eng_cor + 1\n",
    "    else:\n",
    "        eng_cor = eng_cor\n",
    "    \n",
    "print(\"Accuracy of Laplace model for identifying English language in model is :: \", eng_cor/len(en_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Laplace model for identifying italian language in model is ::  0.875\n"
     ]
    }
   ],
   "source": [
    "# Testing model for Italian language\n",
    "eng_cor = 0\n",
    "itl_cor = 0\n",
    "for words in  it_words:\n",
    "    pbl_eng = 1\n",
    "    pbl_itl = 1\n",
    "    for i in range(len(words)):\n",
    "        if(i< len(words)-1):\n",
    "            pbl_eng = pbl_eng * lpc_eng.score(words[i+1],[words[i]])\n",
    "            pbl_itl = pbl_itl * lpc_itl.score(words[i+1],[words[i]])\n",
    "            \n",
    "#        print(\"Word is :: \",words)\n",
    "#        print(\"Prob of eng\",pbl_eng)\n",
    "#        print(\"Prob of itl\",pbl_itl)\n",
    "    \n",
    "    if(pbl_eng <= pbl_itl):\n",
    "        itl_cor = itl_cor + 1\n",
    "    else:\n",
    "        itl_cor = itl_cor\n",
    "    \n",
    "print(\"Accuracy of Laplace model for identifying italian language in model is :: \", itl_cor/len(it_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of unknow characters in italian ::  0.008547008547008548\n",
      "Score of unknow characters in italian ::  0.008547008547008548\n"
     ]
    }
   ],
   "source": [
    "# Print score for unkown elements in Laplace function for english language\n",
    "print(\"Score of unknow characters in italian :: \", lpc_eng.score('!!', ['<UNK>']))\n",
    "print(\"Score of unknow characters in italian :: \", lpc_eng.score('-', ['<UNK>'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of unknow characters in English ::  0.008264462809917356\n",
      "Score of unknow characters in English ::  0.008264462809917356\n"
     ]
    }
   ],
   "source": [
    "# Print score for unkown elements in Laplace function for italian language\n",
    "print(\"Score of unknow characters in English :: \", lpc_itl.score('!!', ['<UNK>']))\n",
    "print(\"Score of unknow characters in English :: \", lpc_itl.score('-', ['<UNK>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Interpolated Language Model \n",
    "n = 2 # number_of_grams\n",
    "\n",
    "train_bge, vocab_bge = padded_everygram_pipeline(2, Etext_train)\n",
    "\n",
    "ipl_eng = KneserNeyInterpolated(n) \n",
    "ipl_eng.fit(train_bge, vocab_bge)\n",
    "\n",
    "Etrain_inter, Evocab_inter =  padded_everygram_pipeline(2, en_train)\n",
    "ipl_itl = KneserNeyInterpolated(n) \n",
    "ipl_itl.fit(Etrain_inter, Evocab_inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of unknow characters in italian ::  0.02564102564102564\n",
      "Score of unknow characters in italian ::  0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Print score for unkown elements in Interpolate function for english language\n",
    "print(\"Score of unknow characters in italian :: \", ipl_eng.score('!!', ['<UNK>']))\n",
    "print(\"Score of unknow characters in italian :: \", ipl_eng.score('-', ['<UNK>'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of unknow characters in English ::  0.017543859649122806\n",
      "Score of unknow characters in English ::  0.017543859649122806\n"
     ]
    }
   ],
   "source": [
    "# Print score for unkown elements in Interpolate function for italian language\n",
    "print(\"Score of unknow characters in English :: \", ipl_itl.score('!!', ['<UNK>']))\n",
    "print(\"Score of unknow characters in English :: \", ipl_itl.score('-', ['<UNK>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the above results I can conclude that Interpolation is giving scores for the characters that are not present in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-2) Text generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "def train_char_lm(fname, order=4):\n",
    "    data = fname\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bigram model\n",
    "lm = train_char_lm(Etext_train, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toke, vocab_toke = padded_everygram_pipeline(2, en_train)\n",
    "\n",
    "# Building laplace model for bigram\n",
    "n = 2\n",
    "lpc_eng = Laplace(n)\n",
    "\n",
    "lpc_eng.fit(train_toke, vocab_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 57 items>\n"
     ]
    }
   ],
   "source": [
    "print(lpc_eng.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lpc_eng.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(lpc_eng, num_words,stext, random_seed):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = stext\n",
    "    for token in lpc_eng.generate(num_words, random_seed):\n",
    "        if token == '<s>':\n",
    "            token='~'\n",
    "        if token == '</s>':\n",
    "            token='~'\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he  've st  a   eabrgyok  'sercka  andrth  ire ll uf  Fiontainddsth qug inou  dyo ofoublld paren sebes\n",
      "ru ngs an  ersskste ofendoutincecacte  s dkinsthins hof oth Whas evoug reliurtalle wirour eslpak  ar o\n",
      "fa , tandoad ed e ive foon   .  off relat ity  teres fonge y   is aioure enstr caintheaskedat r vis fo\n",
      "in blore  dd bit l n ssng   wil t  Por toan ont her  mer ie  y f gann  d ru esutangr  opla  Whe thess \n",
      "yo0  ub ooroisereclof u 'sarbl   , imat  d  Whecererinve  atath  andsind  Pande ct e  theghin  we ia t\n"
     ]
    }
   ],
   "source": [
    "# Bigram model to print sentences with different words\n",
    "inp_list = {'100' : 'he' , '250': 'ru' , '370': 'fa', '820': 'in', '103': 'yo'}\n",
    "for i in inp_list:\n",
    "    vj = generate_sent(lpc_eng, 100,[inp_list[i]], random_seed=i)\n",
    "    vj1 = vj.replace(' ','')\n",
    "    vj2 = vj1.replace('~',' ')\n",
    "    print(vj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 58 items>\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train_tgi, vocab_tri = padded_everygram_pipeline(3, Efull)\n",
    "\n",
    "# Building laplace model for bigram\n",
    "Eng_lap_trigram = Laplace(3)\n",
    "\n",
    "Eng_lap_trigram.fit(train_tgi, vocab_tri)\n",
    "print(Eng_lap_trigram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hel s ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(Eng_lap_trigram, 100,['hel'], '59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hel  Nobods                                                                                            \n",
      "run Takien                                                                                             \n",
      "fas des                                                                                                \n",
      "invng                                                                                                  \n",
      "youd                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "# Trigram model to print sentences with different words\n",
    "inp_list = {'59' : 'hel' , '19': 'run' , '89': 'fas', '99': 'inv', '69': 'you'}\n",
    "for i in inp_list:\n",
    "    vj = generate_sent(Eng_lap_trigram, 100,[inp_list[i]], random_seed=i)\n",
    "    vj1 = vj.replace(' ','')\n",
    "    vj2 = vj1.replace('~',' ')\n",
    "    print(vj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an  ci gha alosso  mechituciuele  , mpelliorentti i dielaira  .  die ni ntrton norande  ize  pa antuza\n",
      "se  pembetelo  ra rno ta ancre porome  dato n   limo vvrsfenostape o  stà nericavi areri    sovera ars\n",
      "ha o le ai vontrtizonondiaiuesomonasimasee  vonnchi loso r  cccegil la ico covvo tttire  l  suno llema\n",
      "un  spri  basparbi faiù domi  iso ra   osue ale etre toprio iona  rioli  ppenve entotro   pe ve utarra\n",
      "ma0 tare anitirengarbe lanialla  Tufisa gad r agrare   e  rgna mbosanersi sttri rl etintate ncelle tto\n"
     ]
    }
   ],
   "source": [
    "# Building laplace model for bigram - Italian language.\n",
    "\n",
    "train_toki, vocab_toki = padded_everygram_pipeline(2, it_train)\n",
    "\n",
    "n = 2\n",
    "lpc_itl = Laplace(n)\n",
    "\n",
    "lpc_itl.fit(train_toki, vocab_toki)\n",
    "\n",
    "# Print output sentence for Italian language using bigram model\n",
    "inp_list = {'100' : 'an' , '250': 'se' , '370': 'ha', '820': 'un', '103': 'ma'}\n",
    "for i in inp_list:\n",
    "    vj = generate_sent(lpc_itl, 100,[inp_list[i]], random_seed=i)\n",
    "    vj1 = vj.replace(' ','')\n",
    "    vj2 = vj1.replace('~',' ')\n",
    "    print(vj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 58 items>\n"
     ]
    }
   ],
   "source": [
    "# Train trigram model for Italian text\n",
    "train_toki, vocab_toki = padded_everygram_pipeline(3, it_train)\n",
    "\n",
    "# Building laplace model for trigram\n",
    "n = 3\n",
    "Itl_lap_trigram = Laplace(n)\n",
    "\n",
    "Itl_lap_trigram.fit(train_toki, vocab_toki)\n",
    "print(Itl_lap_trigram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anc Non il la siticampliare . In presa è una . Il moltando cole offrono avano quantima nuti essere le i\n",
      "se Non darsi dellevata corri . Un effettergere nuti elementi der aiutare . Adultivo ario dei vasi per \n",
      "hai Non sarà sono passere danno scente . Ossere lei ripositivameno inizia è il prendente di rispetto lo\n",
      "un Non di sare che fissatrire sedi anali , mangia è un un convece più temprentra . Ossere è un donne i\n",
      "mar Non super batta delici piace per tuo che lo scriva da amici bisogno meetingre altrati viene danno a\n"
     ]
    }
   ],
   "source": [
    "# Print output sentence for Italian language using trigram model\n",
    "inp_list = {'100' : 'anc' , '250': 'se' , '370': 'hai', '820': 'un', '103': 'mar'}\n",
    "for i in inp_list:\n",
    "    vj = generate_sent(Itl_lap_trigram, 100,[inp_list[i]], random_seed=i)\n",
    "    vj1 = vj.replace(' ','')\n",
    "    vj2 = vj1.replace('~',' ')\n",
    "    print(vj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
